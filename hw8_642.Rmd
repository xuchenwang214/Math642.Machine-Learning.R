---
title: "hw8_642"
author: "Xuchen Wang"
date: "March 28, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```

#### Problem 3
#### (a)
```{r}
x1 <- c(1,1,0,5,6,4)
x2 <- c(4,3,4,1,2,0)
x <- data.frame(x1,x2)
ggplot(data = x)+
  geom_point(aes(x=x1,y=x2))
```

#### (b) Randomly assign a cluster label to each observation. You can use the sample() command in R to do this. Report the cluster labels for each observation.
```{r}
set.seed(3)
set1 <- sample(6,3)
set2 <- -set1
label <- rep(NA,6)
label[set1] <- 1
label[set2] <- 2
y <- cbind(x,label)
y
```

#### (c) Compute the centroid for each cluster.
```{r}
cen1 <- colMeans(x[set1,])
cen2 <- colMeans(x[set2,])
print(list(centroid1=cen1, centroid2=cen2))
```

#### (d) Assign each observation to the centroid to which it is closest, in terms of Euclidean distance. Report the cluster labels for each observation.
```{r}
disvec <- function(x,y){
  return(sum((x-y)^2))
}
set1 <- c()
set2 <- c()
for(i in 1:6){
  dis1 <- disvec(x[i,], cen1)
  dis2 <- disvec(x[i,], cen2)
  if (dis1 < dis2){set1 <- c(set1,i)}
  else{set2 <- c(set2,i)}
}
label[set1] <- 1
label[set2] <- 2
y <- cbind(x,label)
y
```

#### (e) Repeat (c) and (d) until the answers obtained stop changing.
```{r}
cen1 <- colMeans(x[set1,])
cen2 <- colMeans(x[set2,])
set1 <- c()
set2 <- c()
for(i in 1:6){
  dis1 <- disvec(x[i,], cen1)
  dis2 <- disvec(x[i,], cen2)
  if (dis1 < dis2){set1 <- c(set1,i)}
  else{set2 <- c(set2,i)}
}
label[set1] <- 1
label[set2] <- 2
y <- cbind(x,label)
y
```

#### (f) In your plot from (a), color the observations according to the cluster labels obtained.
```{r}
ggplot(data = y)+
  geom_point(aes(x=x1,y=x2,color=as.factor(label)))
```


#### Problem 9
#### (a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.
```{r}
attach(USArrests)
us_hcl <- hclust(dist(USArrests), method = "complete")
plot(us_hcl)
```

#### (b) Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?
```{r}
cutree(us_hcl,3)
table(cutree(us_hcl,3))
```

#### (c) Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.
```{r}
USArrests_sc <- scale(USArrests, center = F, scale = T)
us_hcl_sc <- hclust(dist(USArrests_sc), method = "complete")
plot(us_hcl_sc)
```

#### (d) What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.
```{r}
cutree(us_hcl_sc,3)
table(cutree(us_hcl_sc,3))
```

I think the variables should be scaled before since the unit of the variables are different. Murder, Assault and Rape are measured in per 100000. And UrbanPop is measured in pertentage.


#### Problem 10
#### (a)
```{r}
X1 <- matrix(rnorm(20*50), ncol = 50)
X2 <- matrix(rnorm(20*50, mean = 10), ncol = 50)
X3 <- matrix(rnorm(20*50, mean = 20), ncol = 50)
X <- rbind(X1,X2,X3)
```

#### (b)
```{r}
pr <- prcomp(X)
summary(pr)
pr$x[,1:2]
y <- rep(1:3, c(20,20,20))
XX <- as.data.frame(cbind(X,y))
ggplot(data = XX)+
  geom_point(aes(x=pr$x[,1],y=pr$x[,2],color=as.factor(y)))
```

#### (c) 
```{r}
set.seed(2)
km <- kmeans(X,3,nstart = 20)
table(km$cluster, y)
```

It matches perfectly.

#### (d) Perform K-means clustering with K = 2. Describe your results.
```{r}
set.seed(3)
km1 <- kmeans(X,2,nstart = 20)
table(km1$cluster, y)
```

All of the previous class assigned to another class.

#### (e) Now perform K-means clustering with K = 4, and describe your results.
```{r}
set.seed(3)
km2 <- kmeans(X,4,nstart = 20)
table(km2$cluster, y)
```

All of one previous class split into two classes.

#### (f) 
```{r}
km_pr <- kmeans(pr$x[,1:2],3,nstart = 20)
km_pr$cluster
table(km_pr$cluster, y)
```

It matches perfectly.

#### (g) 
```{r}
X_sc <- scale(X,center = F,scale = T)
set.seed(3)
km_sc <- kmeans(X_sc,3,nstart = 20)
table(km_sc$cluster,y)
```

It has no effect for this example. Because when I generate the observations, I suppose their standard deviation is 1.





























